# @package _global_

optimizer:
  groups:
    - pattern: ^embedding.bert
      lr: 3.5e-5

  args:
    _target_: torch.optim.AdamW
    lr: 3.5e-3
    betas: [ 0.9, 0.9 ]
    weight_decay: 0.
    eps: 1.0e-12

scheduler:
  interval: step
  frequency: 1
  init_when_running: true
  args:
    _target_: transformers.get_linear_schedule_with_warmup
    num_warmup_steps: '1 epoch'
    num_training_steps: '20 epoch'

