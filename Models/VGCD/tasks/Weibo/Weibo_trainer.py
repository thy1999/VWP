#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
@file  : Weibo_trainer.py
@author: xiaoya li
@contact : xiaoya_li@shannonai.com
@date  : 2021/06/30 17:28
@version: 1.0
@desc  :
"""

import os
import re
import json
import argparse
import logging
from functools import partial
from collections import namedtuple

from datasets.collate_functions import collate_to_max_length
from datasets.weibo_ner_dataset import WeiboNERDataset
from models.modeling_glycebert import GlyceBertForTokenClassification
from utils.random_seed import set_random_seed
from metrics.ner import SpanF1ForNER
from metrics.ner_pos import cws_SpanF1ForNER

# enable reproducibility
# https://pytorch-lightning.readthedocs.io/en/latest/trainer.html
set_random_seed(2333)

import torch
from torch.nn import functional as F
from torch.nn.modules import CrossEntropyLoss
from torch.utils.data.dataloader import DataLoader, RandomSampler, SequentialSampler
from transformers import AdamW, BertConfig, get_linear_schedule_with_warmup

import pytorch_lightning as pl
from pytorch_lightning import Trainer
from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint
from pytorch_lightning.loggers import TensorBoardLogger


class WeiboTask(pl.LightningModule):
#è¯¥æ¨¡å—ç»§æ‰¿è‡ªpl.LightningModuleï¼Œå®ƒå®žçŽ°äº†PyTorch Lightningä¸­è§„å®šçš„è®­ç»ƒå’ŒéªŒè¯ç­‰æ­¥éª¤ã€‚

#è¿™ä¸ªæ¨¡å—å®žçŽ°äº†NERä»»åŠ¡çš„è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•è¿‡ç¨‹ï¼Œå¹¶ä¸”èƒ½å¤Ÿå°†ç»“æžœä¿å­˜åˆ°æ–‡ä»¶ä¸­ä»¥ä¾›è¿›ä¸€æ­¥åˆ†æžã€‚
# å®ƒç»“åˆäº†PyTorch Lightningçš„è®­ç»ƒæ¡†æž¶å’ŒHugging Face Transformersåº“çš„BERTæ¨¡åž‹ï¼Œæ–¹ä¾¿å®žçŽ°å¾®åšNERä»»åŠ¡çš„å¿«é€Ÿå¼€å‘å’Œè°ƒè¯•ã€‚

    def __init__(self, args: argparse.Namespace):
        """Initialize a model, tokenizer and config"""
        #åˆå§‹åŒ–å‡½æ•°ï¼Œç”¨äºŽåˆ›å»ºå¹¶é…ç½®æ¨¡åž‹ã€‚å‚æ•°argsæ˜¯ä¸€ä¸ªå‘½åç©ºé—´ï¼ˆargparse.Namespaceï¼‰å¯¹è±¡ï¼Œç”¨äºŽä¿å­˜è®­ç»ƒæ—¶çš„å‚æ•°é…ç½®ã€‚

        super().__init__()
        self.args = args
        if isinstance(args, argparse.Namespace):
            self.save_hyperparameters(args)
        else:
            TmpArgs = namedtuple("tmp_args", field_names=list(args.keys()))
            self.args = args = TmpArgs(**args)

        self.entity_labels = WeiboNERDataset.get_labels(os.path.join(args.data_dir, "ner_labels.txt"))
        self.bert_dir = args.bert_path
        self.num_labels = len(self.entity_labels)
        self.bert_config = BertConfig.from_pretrained(self.bert_dir, output_hidden_states=True,
                                                      num_labels=self.num_labels,
                                                      hidden_dropout_prob=self.args.hidden_dropout_prob)
        #æ ¹æ®é¢„è®­ç»ƒæ¨¡åž‹çš„è·¯å¾„ã€æ˜¯å¦è¾“å‡ºéšè—çŠ¶æ€ã€æ ‡ç­¾æ•°é‡å’Œéšè—å±‚ä¸¢å¼ƒæ¦‚çŽ‡ç­‰å‚æ•°ï¼Œåˆ›å»ºå¹¶é…ç½®ä¸€ä¸ªBERTæ¨¡åž‹çš„é…ç½®å¯¹è±¡ã€‚
        
        self.model = GlyceBertForTokenClassification.from_pretrained(self.bert_dir,
                                                                     config=self.bert_config,
                                                                     mlp=False if self.args.classifier=="single" else True)
        #æ ¹æ®é¢„è®­ç»ƒæ¨¡åž‹çš„è·¯å¾„ã€é…ç½®å¯¹è±¡å’Œåˆ†ç±»å™¨ç±»åž‹ï¼Œåˆ›å»ºå¹¶é…ç½®ä¸€ä¸ªé¢„è®­ç»ƒçš„GlyceBERTæ¨¡åž‹ã€‚

        self.ner_evaluation_metric = SpanF1ForNER(entity_labels=self.entity_labels, save_prediction=self.args.save_ner_prediction)
        #å®ƒç”¨äºŽè®¡ç®—å‘½åå®žä½“è¯†åˆ«ä»»åŠ¡ä¸­çš„è·¨åº¦çº§åˆ«ï¼ˆspan-levelï¼‰çš„F1åˆ†æ•°ã€‚

        self.cws_ner_evaluation_metric = cws_SpanF1ForNER(entity_labels=self.entity_labels, save_prediction=self.args.save_ner_prediction)
        
        format = '%(asctime)s - %(name)s - %(message)s'
        #å®šä¹‰äº†æ—¥å¿—è®°å½•çš„æ ¼å¼å­—ç¬¦ä¸²ã€‚å®ƒåŒ…å«ä¸‰ä¸ªå ä½ç¬¦ï¼Œåˆ†åˆ«è¡¨ç¤ºè®°å½•çš„æ—¶é—´ã€è®°å½•å™¨çš„åç§°å’Œè®°å½•çš„æ¶ˆæ¯ã€‚

        logging.basicConfig(format=format, filename=os.path.join(self.args.save_path, "eval_result_log.txt"), level=logging.INFO)
        #é…ç½®æ—¥å¿—è®°å½•å™¨çš„åŸºæœ¬è®¾ç½®ã€‚

        self.result_logger = logging.getLogger(__name__)
        #èŽ·å–ä¸€ä¸ªåä¸º__name__çš„æ—¥å¿—è®°å½•å™¨ã€‚

        self.result_logger.setLevel(logging.INFO)
        #è®¾ç½®æ—¥å¿—è®°å½•å™¨çš„çº§åˆ«ä¸ºINFOï¼Œè¡¨ç¤ºåªè®°å½•INFOçº§åˆ«åŠä»¥ä¸Šçš„æ—¥å¿—æ¶ˆæ¯ã€‚

    def configure_optimizers(self):
        """Prepare optimizer and schedule (linear warmup and decay)"""
        #é…ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ çŽ‡è°ƒåº¦å™¨ï¼Œå¹¶è®¾ç½®çº¿æ€§å­¦ä¹ çŽ‡é¢„çƒ­å’Œè¡°å‡ã€‚

        no_decay = ["bias", "LayerNorm.weight"]
        optimizer_grouped_parameters = [
            {
                "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
                "weight_decay": self.args.weight_decay,
            },
            {
                "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
                "weight_decay": 0.0,
            },
        ]

        if self.args.optimizer == "adamw":
            optimizer = AdamW(optimizer_grouped_parameters, betas=(0.9, 0.999), lr=self.args.lr, eps=self.args.adam_epsilon, )
        elif self.args.optimizer == "torch.adam":
            optimizer = torch.optim.AdamW(optimizer_grouped_parameters,
                                          lr=self.args.lr,
                                          eps=self.args.adam_epsilon,
                                          weight_decay=self.args.weight_decay)
        else:
            raise ValueError("Please import the Optimizer first. ")
        num_gpus = len([x for x in str(self.args.gpus).split(",") if x.strip()])
        t_total = (len(self.train_dataloader()) // (
                self.args.accumulate_grad_batches * num_gpus) + 1) * self.args.max_epochs
        warmup_steps = int(self.args.warmup_proportion * t_total)
        if self.args.no_lr_scheduler:
            return [optimizer]
        else:
            scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)
            return [optimizer], [{"scheduler": scheduler, "interval": "step"}]

    def forward(self, input_ids, pinyin_ids, batch_images_list):
        #æ¨¡åž‹å‰å‘ä¼ æ’­å‡½æ•°ã€‚

        attention_mask = (input_ids != 0).long()
        return self.model(input_ids, pinyin_ids, batch_images_list, attention_mask=attention_mask)

    def compute_loss(self, logits, labels, loss_mask=None):
        #è®¡ç®—äº¤å‰ç†µæŸå¤±ã€‚
        """
        Desc:
            compute cross entropy loss
        Args:
            logits: FloatTensor, shape of [batch_size, sequence_len, num_labels]
            labels: LongTensor, shape of [batch_size, sequence_len, num_labels]
            loss_mask: Optional[LongTensor], shape of [batch_size, sequence_len].
                1 for non-PAD tokens, 0 for PAD tokens.
        """
        loss_fct = CrossEntropyLoss()
        if loss_mask is not None:
            active_loss = loss_mask.view(-1) == 1
            active_logits = logits.view(-1, self.num_labels)
            active_labels = torch.where(
                active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)
            )
            loss = loss_fct(active_logits, active_labels)
        else:
            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
        return loss

    def training_step(self, batch, batch_idx):
        # è®­ç»ƒè¿‡ç¨‹ä¸­çš„å•æ­¥å‡½æ•°ï¼Œç”¨äºŽè®¡ç®—è®­ç»ƒæŸå¤±ã€‚æŽ¥æ”¶ä¸€ä¸ªæ‰¹æ¬¡æ•°æ®batchå’Œæ‰¹æ¬¡ç´¢å¼•batch_idxï¼Œè¿”å›žæŸå¤±å€¼å’Œå…¶ä»–éœ€è¦è®°å½•çš„æ—¥å¿—ä¿¡æ¯ã€‚
        batch_images_list = batch[1]
        input_ids, pinyin_ids, labels = batch[0]
        #ä»Žæ‰¹æ¬¡æ•°æ®batchä¸­è§£åŒ…å¾—åˆ°è¾“å…¥IDã€æ‹¼éŸ³IDå’Œæ ‡ç­¾ã€‚

        loss_mask = (input_ids != 0).long()
        #åˆ›å»ºä¸€ä¸ªæŸå¤±æŽ©ç loss_maskï¼Œå°†éžé›¶å…ƒç´ è®¾ä¸º1ï¼Œé›¶å…ƒç´ è®¾ä¸º0ã€‚

        batch_size, seq_len = input_ids.shape
        #èŽ·å–è¾“å…¥IDçš„å½¢çŠ¶ï¼Œå¾—åˆ°æ‰¹æ¬¡å¤§å°batch_sizeå’Œåºåˆ—é•¿åº¦seq_lenã€‚

        pinyin_ids = pinyin_ids.view(batch_size, seq_len, 8)
        #è°ƒæ•´æ‹¼éŸ³IDçš„å½¢çŠ¶ï¼Œå°†å…¶è§†ä¸ºå½¢çŠ¶ä¸º(batch_size, seq_len, 8)çš„ä¸‰ç»´å¼ é‡ã€‚
        #è¿™é‡Œå‡è®¾æ‹¼éŸ³IDæ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º(batch_size, seq_len*8)çš„äºŒç»´å¼ é‡ï¼Œå°†å…¶è½¬æ¢ä¸ºä¸‰ç»´å¼ é‡ï¼Œæ¯ä¸ªåºåˆ—ä½ç½®æœ‰8ä¸ªæ‹¼éŸ³ç‰¹å¾ã€‚

        sequence_logits = self.forward(input_ids=input_ids, pinyin_ids=pinyin_ids, batch_images_list=batch_images_list)[0]
        #è°ƒç”¨æ¨¡åž‹çš„å‰å‘ä¼ æ’­æ–¹æ³•forwardï¼Œä¼ å…¥è¾“å…¥IDå’Œæ‹¼éŸ³IDï¼ŒèŽ·å–åºåˆ—çš„é¢„æµ‹logitsã€‚

        loss = self.compute_loss(sequence_logits, labels, loss_mask=loss_mask)
        #è®¡ç®—æŸå¤±å€¼ï¼Œè°ƒç”¨compute_lossæ–¹æ³•ï¼Œä¼ å…¥åºåˆ—logitsã€æ ‡ç­¾å’ŒæŸå¤±æŽ©ç ï¼ŒèŽ·å–æŸå¤±å€¼ã€‚

        tf_board_logs = {
            "train_loss": loss,
            "lr": self.trainer.optimizers[0].param_groups[0]["lr"]
        }
        #åˆ›å»ºä¸€ä¸ªå­—å…¸tf_board_logsï¼ŒåŒ…å«éœ€è¦è®°å½•åˆ°TensorBoardçš„æ—¥å¿—ä¿¡æ¯ã€‚è¿™é‡Œè®°å½•äº†è®­ç»ƒæŸå¤±å’Œå­¦ä¹ çŽ‡ã€‚

        return {"loss": loss, "log": tf_board_logs}
        #è¿”å›žä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«æŸå¤±å€¼å’Œéœ€è¦è®°å½•çš„æ—¥å¿—ä¿¡æ¯ã€‚è¿™ä¸ªå­—å…¸å°†è¢«ä¼ é€’ç»™è®­ç»ƒå™¨ï¼ˆtrainerï¼‰ï¼Œç”¨äºŽæ—¥å¿—è®°å½•å’Œä¼˜åŒ–å™¨æ›´æ–°ã€‚

    def validation_step(self, batch, batch_idx):
        #éªŒè¯è¿‡ç¨‹ä¸­çš„å•æ­¥å‡½æ•°ï¼Œç”¨äºŽè®¡ç®—éªŒè¯æŸå¤±å’Œè¯„ä¼°æŒ‡æ ‡ã€‚æŽ¥æ”¶ä¸€ä¸ªæ‰¹æ¬¡æ•°æ®batchå’Œæ‰¹æ¬¡ç´¢å¼•batch_idxï¼Œè¿”å›žéªŒè¯æŸå¤±å’Œè¯„ä¼°æŒ‡æ ‡ã€‚
        batch_images_list = batch[1]
        input_ids, pinyin_ids, gold_labels = batch[0]
        batch_size, seq_len = input_ids.shape
        loss_mask = (input_ids != 0).long()
        pinyin_ids = pinyin_ids.view(batch_size, seq_len, 8)
        sequence_logits = self.forward(input_ids=input_ids, pinyin_ids=pinyin_ids, batch_images_list=batch_images_list)[0]
        loss = self.compute_loss(sequence_logits, gold_labels, loss_mask=loss_mask)
        probabilities, argmax_labels = self.postprocess_logits_to_labels(sequence_logits.view(batch_size, seq_len, -1))
        confusion_matrix = self.ner_evaluation_metric(argmax_labels, gold_labels, sequence_mask=loss_mask)
        return {"val_loss": loss, "confusion_matrix": confusion_matrix}

    def validation_epoch_end(self, outputs):
        #éªŒè¯è¿‡ç¨‹ç»“æŸåŽçš„å›žè°ƒå‡½æ•°ï¼Œç”¨äºŽè®¡ç®—å¹³å‡éªŒè¯æŸå¤±å’Œè¯„ä¼°æŒ‡æ ‡ã€‚æŽ¥æ”¶ä¸€ä¸ªåŒ…å«æ‰€æœ‰validation_stepè¿”å›žå€¼çš„åˆ—è¡¨outputsï¼Œè¿”å›žä¸€ä¸ªåŒ…å«éªŒè¯æŸå¤±å’Œè¯„ä¼°æŒ‡æ ‡çš„å­—å…¸ã€‚
        # avg_loss = torch.stack([x["val_loss"] for x in outputs]).mean()
        # confusion_matrix = torch.stack([x[f"confusion_matrix"] for x in outputs]).sum(0)
        # all_true_positive, all_false_positive, all_false_negative = confusion_matrix
        # precision, recall, f1 = self.ner_evaluation_metric.compute_f1_using_confusion_matrix(all_true_positive, all_false_positive, all_false_negative)

        # self.result_logger.info(f"EVAL INFO -> current_epoch is: {self.trainer.current_epoch}, current_global_step is: {self.trainer.global_step} ")
        # self.result_logger.info(f"EVAL INFO -> valid_f1 is: {f1}")
        # tensorboard_logs = {"val_loss": avg_loss, "val_f1": f1, }
        # return {"val_loss": avg_loss, "val_log": tensorboard_logs, "val_f1": f1, "val_precision": precision, "val_recall": recall}

        avg_loss = torch.stack([x["val_loss"] for x in outputs]).mean()
        confusion_matrix = torch.stack([x["confusion_matrix"] for x in outputs]).sum(0)
        all_true_positive, all_false_positive, all_false_negative = confusion_matrix
        
        if self.args.save_ner_prediction:
            precision, recall, f1, entity_tuple = self.ner_evaluation_metric.compute_f1_using_confusion_matrix(all_true_positive, all_false_positive, all_false_negative, prefix="dev")    
            gold_entity_lst, pred_entity_lst = entity_tuple
            self.save_predictions_to_file(gold_entity_lst, pred_entity_lst, prefix="dev")
        else:
            precision, recall, f1 = self.ner_evaluation_metric.compute_f1_using_confusion_matrix(all_true_positive, all_false_positive, all_false_negative)

        self.result_logger.info(f"EVAL INFO -> current_epoch is: {self.trainer.current_epoch}, current_global_step is: {self.trainer.global_step} ")
        self.result_logger.info(f"EVAL INFO -> valid_f1 is: {f1}, precision is: {precision}, recall is: {recall}")
        print("=="*50)
        print("æ­£åœ¨è¯„ä»·devä¸Šçš„ç»“æžœï¼š")
        print((f"EVAL INFO -> current_epoch is: {self.trainer.current_epoch}, current_global_step is: {self.trainer.global_step} "))
        print(f"DEV RESULT -> DEV F1: {f1}, Precision: {precision}, Recall: {recall} ")
        print("=="*50)
        tensorboard_logs = {"val_loss": avg_loss, "val_f1": f1, }
        return {"val_loss": avg_loss, "val_log": tensorboard_logs, "val_f1": f1, "val_precision": precision, "val_recall": recall}

    def train_dataloader(self,) -> DataLoader:
        #èŽ·å–è®­ç»ƒæ•°æ®åŠ è½½å™¨ã€‚
        return self.get_dataloader("train")

    def val_dataloader(self, ) -> DataLoader:
        #: èŽ·å–éªŒè¯æ•°æ®åŠ è½½å™¨ã€‚
        return self.get_dataloader("dev")

    def _load_dataset(self, prefix="test"):
        dataset = WeiboNERDataset(directory=self.args.data_dir, prefix=prefix,
                                    vocab_file=os.path.join(self.args.bert_path, "vocab.txt"),
                                    max_length=self.args.max_length,
                                    config_path=os.path.join(self.args.bert_path, "config"),
                                    file_name=self.args.file_name, task_name=self.args.task_name)

        return dataset

    def get_dataloader(self, prefix="train", limit=None) -> DataLoader:
        """return {train/dev/test} dataloader"""
        #æ ¹æ®å‰ç¼€ï¼ˆ"train"ã€"dev"æˆ–"test"ï¼‰èŽ·å–ç›¸åº”æ•°æ®åŠ è½½å™¨ã€‚

        dataset = self._load_dataset(prefix=prefix)

        if prefix == "train":
            batch_size = self.args.train_batch_size
            # small dataset like weibo ner, define data_generator will help experiment reproducibility.
            data_generator = torch.Generator()
            data_generator.manual_seed(self.args.seed)
            data_sampler = RandomSampler(dataset, generator=data_generator)
        else:
            batch_size = self.args.eval_batch_size
            data_sampler = SequentialSampler(dataset)

        # sampler option is mutually exclusive with shuffle
        dataloader = DataLoader(dataset=dataset, sampler=data_sampler, batch_size=batch_size,
                                num_workers=self.args.workers, collate_fn=partial(collate_to_max_length, fill_values=[0, 0, 0]),
                                drop_last=False)

        return dataloader

    def test_dataloader(self, ) -> DataLoader:
        #: èŽ·å–æµ‹è¯•æ•°æ®åŠ è½½å™¨ã€‚

        return self.get_dataloader("test")

    def test_step(self, batch, batch_idx):
        #æµ‹è¯•è¿‡ç¨‹ä¸­çš„å•æ­¥å‡½æ•°ï¼Œç”¨äºŽè®¡ç®—æµ‹è¯•è¯„ä¼°æŒ‡æ ‡ã€‚æŽ¥æ”¶ä¸€ä¸ªæ‰¹æ¬¡æ•°æ®batchå’Œæ‰¹æ¬¡ç´¢å¼•batch_idxï¼Œè¿”å›žæµ‹è¯•è¯„ä¼°æŒ‡æ ‡ã€‚
        batch_images_list = batch[1]
        input_ids, pinyin_ids, gold_labels = batch[0]
        sequence_mask = (input_ids != 0).long()
        batch_size, seq_len = input_ids.shape
        pinyin_ids = pinyin_ids.view(batch_size, seq_len, 8)
        sequence_logits = self.forward(input_ids=input_ids, pinyin_ids=pinyin_ids,batch_images_list=batch_images_list)[0]
        probabilities, argmax_labels = self.postprocess_logits_to_labels(sequence_logits.view(batch_size, seq_len, -1))
        confusion_matrix = self.ner_evaluation_metric(argmax_labels, gold_labels, sequence_mask=sequence_mask)
        cws_confusion_matrix = self.cws_ner_evaluation_metric(argmax_labels, gold_labels, sequence_mask=sequence_mask)

        return {"confusion_matrix": confusion_matrix, "cws_confusion_matrix":cws_confusion_matrix}


    def test_epoch_end(self, outputs):
        #æµ‹è¯•è¿‡ç¨‹ç»“æŸåŽçš„å›žè°ƒå‡½æ•°ï¼Œç”¨äºŽè®¡ç®—æµ‹è¯•è¯„ä¼°æŒ‡æ ‡ã€‚æŽ¥æ”¶ä¸€ä¸ªåŒ…å«æ‰€æœ‰test_stepè¿”å›žå€¼çš„åˆ—è¡¨outputsï¼Œè¿”å›žä¸€ä¸ªåŒ…å«æµ‹è¯•è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸ã€‚

        confusion_matrix = torch.stack([x[f"confusion_matrix"] for x in outputs]).sum(0)
        all_true_positive, all_false_positive, all_false_negative = confusion_matrix
        cws_confusion_matrix = torch.stack([x[f"cws_confusion_matrix"] for x in outputs]).sum(0)
        cws_all_true_positive, cws_all_false_positive, cws_all_false_negative = cws_confusion_matrix

        if self.args.save_ner_prediction:
            precision, recall, f1, entity_tuple = self.ner_evaluation_metric.compute_f1_using_confusion_matrix(all_true_positive, all_false_positive, all_false_negative, prefix="test")
            cws_precision, cws_recall, cws_f1, cws_entity_tuple = self.cws_ner_evaluation_metric.compute_f1_using_confusion_matrix(cws_all_true_positive, cws_all_false_positive, cws_all_false_negative, prefix="test")
            gold_entity_lst, pred_entity_lst = entity_tuple
            self.save_predictions_to_file(gold_entity_lst, pred_entity_lst)
        else:
            precision, recall, f1 = self.ner_evaluation_metric.compute_f1_using_confusion_matrix(all_true_positive, all_false_positive, all_false_negative)

        tensorboard_logs = {"test_f1": f1,}
        self.result_logger.info(f"TEST RESULT -> TEST F1: {f1}, Precision: {precision}, Recall: {recall} ")
        tensorboard_logs = {"CWS_test_f1": cws_f1,}
        self.result_logger.info(f"CWS_TEST RESULT -> CWS_TEST F1: {cws_f1}, CWS_Precision: {cws_precision}, CWS_Recall: {cws_recall} ")


        print("=="*50)
        print("æ­£åœ¨è¯„ä»·testä¸Šçš„ç»“æžœï¼š")
        print(f"TEST RESULT -> TEST F1: {f1}, Precision: {precision}, Recall: {recall} ")
        print(f"CWS_TEST RESULT -> CWS_TEST F1: {cws_f1}, CWS_Precision: {cws_precision}, CWS_Recall: {cws_recall} ")
        
        print("=="*50)
        return {"test_log": tensorboard_logs, "test_f1": f1, "test_precision": precision, "test_recall": recall}

    def postprocess_logits_to_labels(self, logits):
        # åŽå¤„ç†å‡½æ•°ï¼Œå°†æ¨¡åž‹è¾“å‡ºçš„logitsè½¬æ¢ä¸ºæ¦‚çŽ‡å’Œé¢„æµ‹æ ‡ç­¾ã€‚

        """input logits should in the shape [batch_size, seq_len, num_labels]"""
        bs, seqlen, num_labels = logits.shape
        mask = torch.zeros((bs, seqlen, num_labels)).to(logits.device)
        mask[:, :, 0] = -float('inf')
        logits = logits + mask        
        probabilities = F.softmax(logits, dim=2) # shape of [batch_size, seq_len, num_labels]
        argmax_labels = torch.argmax(probabilities, 2, keepdim=False) # shape of [batch_size, seq_len]
        return probabilities, argmax_labels

    def save_predictions_to_file(self, gold_entity_lst, pred_entity_lst, prefix="test"):
        # å°†NERä»»åŠ¡çš„é¢„æµ‹ç»“æžœä¿å­˜åˆ°æ–‡ä»¶ä¸­ï¼Œç”¨äºŽåŽç»­åˆ†æžå’ŒæŸ¥çœ‹ã€‚

        dataset = self._load_dataset(prefix=prefix)
        data_items = dataset.data_items
        if prefix=="test":
            file_path = "/public/home/dzhang/pyProject/hytian/ChineseBert_ADD_PIC_WEIBO/mertic/test_raw.txt"  # è¯·å°†æ–‡ä»¶è·¯å¾„æ›¿æ¢ä¸ºå®žé™…æ–‡ä»¶çš„è·¯å¾„
            save_file_path = os.path.join(self.args.save_path, "test_predictions.txt")
        else:
            file_path = "/public/home/dzhang/pyProject/hytian/ChineseBert_ADD_PIC_WEIBO/mertic/dev_raw.txt"
            save_file_path = os.path.join(self.args.save_path, "dev_predictions.txt")
        text_list = []
        with open(file_path, "r", encoding="utf-8") as file:
            for line in file:
                text_list.append(line.strip().split())  # ä½¿ç”¨ strip() åŽ»é™¤æ¯è¡Œæœ«å°¾çš„æ¢è¡Œç¬¦
        orig_text_list = []
        orig_gold_entity_lst = []
        for res in data_items:
            orig_text_list.append(res[0])
            orig_gold_entity_lst.append(res[1])

        print(f"INFO -> write predictions to {save_file_path}")
        with open(save_file_path, "w") as f:
            for gold_label_item, pred_label_item, text_list_item, orig_text_item, orig_gold_label_item in zip(gold_entity_lst, pred_entity_lst, text_list, orig_text_list, orig_gold_entity_lst):
                assert len(gold_label_item) == len(pred_label_item) == len(text_list_item)
                t = 0
                l = 0
                new_gold_label = []
                new_pred_label = []
                while t < len(orig_text_item):
                    ori_char = orig_text_item[t]
                    if ori_char == 'ë…„' or ori_char == 'ì›”' or ori_char == 'ì¼':
                        new_gold_label.append(orig_gold_label_item[t])
                        new_pred_label.append(orig_gold_label_item[t])
                        t+=1
                        l+=3
                        continue
                    if orig_text_item[t]=='ï¸' and (orig_text_item[t-1] in ['â¤','âœˆ','â™‚','â˜•','âœ–']):
                        new_gold_label.append(orig_gold_label_item[t])
                        new_pred_label.append(orig_gold_label_item[t])
                        t+=1
                        continue
                    enc_char = text_list_item[l]
                    if ori_char.lower() == enc_char:
                        new_gold_label.append(gold_label_item[l])
                        new_pred_label.append(pred_label_item[l])
                        t+=1
                        l+=1
                    else:
                        if enc_char=="UNK":
                            temp_gold = gold_label_item[l]
                            temp_pred = pred_label_item[l]
                            if "S-" in temp_gold:
                                new_gold_label.append(temp_gold)
                                new_pred_label.append(temp_pred)
                                t+=1
                                l+=1
                            else:
                                if ori_char=='è²':
                                    new_gold_label.append(temp_gold)
                                    new_pred_label.append(temp_pred)
                                    t+=1
                                    l+=1
                                    continue
                                n = t
                                while "E-" not in orig_gold_label_item[n]:
                                    n+=1
                                if "B-" in orig_gold_label_item[t]:
                                    new_gold_label.append("B-"+temp_gold.split("-")[1])
                                    for _ in range(t+1,n):
                                        new_gold_label.append("M-"+temp_gold.split("-")[1])
                                    new_gold_label.append("E-"+temp_gold.split("-")[1])
                                    
                                    new_pred_label.append("B-"+temp_pred.split("-")[1])
                                    for _ in range(t+1,n):
                                        new_pred_label.append("M-"+temp_pred.split("-")[1])
                                    new_pred_label.append("E-"+temp_pred.split("-")[1])
                                
                                elif "M-" in orig_gold_label_item[t]:
                                    for _ in range(t,n):
                                        new_gold_label.append("M-"+temp_gold.split("-")[1])
                                    new_gold_label.append("E-"+temp_gold.split("-")[1])  
                                    for _ in range(t,n):
                                        new_pred_label.append("M-"+temp_pred.split("-")[1])
                                    new_pred_label.append("E-"+temp_pred.split("-")[1])
                                else:
                                    new_gold_label.append("E-"+temp_gold.split("-")[1])  
                                    new_pred_label.append("E-"+temp_pred.split("-")[1])
                                                              
                                if ori_char=="Ê•":
                                    l += (n-t+1)
                                elif ori_char=="ð˜":
                                    l += 3
                                else:
                                    l += 1
                                t = n+1
                        else:
                            if "##" in enc_char:
                                enc_char = enc_char.replace("##","")
                            temp_gold = gold_label_item[l]
                            temp_pred = pred_label_item[l]
                            n = len(enc_char)
                            if n==1:
                                # if orig_text_item[t]=='ï¸' and (orig_text_item[t-1] in ['â¤','âœˆ','â™‚']):
                                #     new_gold_label.append("E-"+orig_gold_label_item[t-1].split("-")[1])
                                #     new_pred_label.append("E-"+orig_gold_label_item[t-1].split("-")[1])
                                #     t+=1
                                # else:
                                new_gold_label.append(gold_label_item[l])
                                new_pred_label.append(pred_label_item[l])
                                t+=1
                                l+=1
                            else:
                                # if enc_char == "".join(orig_text_item[t:t+n]).lower():
                                if temp_gold.split("-")[0]=="B":
                                    new_gold_label.append("B-"+temp_gold.split("-")[1])
                                    for i in range(n-1):
                                        new_gold_label.append("M-"+temp_gold.split("-")[1])
                                elif temp_gold.split("-")[0]=="M":
                                    for i in range(n):
                                        new_gold_label.append("M-"+temp_gold.split("-")[1])
                                elif temp_gold.split("-")[0]=="E":
                                    for i in range(n-1):
                                        new_gold_label.append("M-"+temp_gold.split("-")[1])
                                    new_gold_label.append("E-"+temp_gold.split("-")[1])
                                else:
                                    new_gold_label.append("B-"+temp_gold.split("-")[1])
                                    for i in range(n-2):
                                        new_gold_label.append("M-"+temp_gold.split("-")[1])
                                    new_gold_label.append("E-"+temp_gold.split("-")[1])
                                
                                if temp_pred.split("-")[0]=="B":
                                    new_pred_label.append("B-"+temp_pred.split("-")[1])
                                    for i in range(n-1):
                                        new_pred_label.append("M-"+temp_pred.split("-")[1])
                                elif temp_pred.split("-")[0]=="M":
                                    for i in range(n):
                                        new_pred_label.append("M-"+temp_pred.split("-")[1])
                                elif temp_pred.split("-")[0]=="E":
                                    for i in range(n-1):
                                        new_pred_label.append("M-"+temp_pred.split("-")[1])
                                    new_pred_label.append("E-"+temp_pred.split("-")[1])
                                else:
                                    new_pred_label.append("B-"+temp_pred.split("-")[1])
                                    for i in range(n-2):
                                        new_pred_label.append("M-"+temp_pred.split("-")[1])
                                    new_pred_label.append("E-"+temp_pred.split("-")[1])
                                l+=1
                                t+=n        
                    assert new_gold_label==orig_gold_label_item[:t]
                    assert len(new_pred_label) == len(new_gold_label) == t
                
                assert new_gold_label == orig_gold_label_item
                assert len(new_gold_label) == len(new_pred_label) == len(orig_gold_label_item) == len(orig_text_item)
                f.write(" ".join(orig_text_item)+"\n")
                sentence=[]
                sentence1=[]
                gold_text_results=""
                pred_text_results=""
                for i in range(len(new_gold_label)):
                    if new_gold_label[i].split("-")[0]== "S" or new_gold_label[i].split("-")[0]=="E":
                        sentence.append(orig_text_item[i])
                        sentence.append(" ")
                    elif new_gold_label[i].split("-")[0]=="B" or new_gold_label[i].split("-")[0]=="M":
                        sentence.append(orig_text_item[i])
                gold_text_results="".join(sentence)
                for i in range(len(new_pred_label)):
                    if new_pred_label[i].split("-")[0]== "S" or new_pred_label[i].split("-")[0]=="E":
                        sentence1.append(orig_text_item[i])
                        sentence1.append(" ")
                    elif new_pred_label[i].split("-")[0]=="B" or new_pred_label[i].split("-")[0]=="M":
                        sentence1.append(orig_text_item[i])
                pred_text_results="".join(sentence1)              
                f.write("gold_text_results: "+gold_text_results+"\n")
                f.write("pred_text_results: "+pred_text_results+"\n")
                f.write("gold_label_item: "+" ".join(new_gold_label)+"\n")
                f.write("pred_label_item: "+" ".join(new_pred_label)+"\n")
                f.write("\n")

def get_parser():
    parser = argparse.ArgumentParser()

    parser.add_argument("--bert_path", type=str, help="bert config file")
    parser.add_argument("--train_batch_size", type=int, default=8, help="batch size")
    parser.add_argument("--eval_batch_size", type=int, default=8, help="batch size")
    parser.add_argument("--lr", type=float, default=2e-5, help="learning rate")
    parser.add_argument("--workers", type=int, default=0, help="num workers for dataloader")
    parser.add_argument("--weight_decay", default=0.0, type=float, help="Weight decay if we apply some.")
    parser.add_argument("--adam_epsilon", default=1e-8, type=float, help="Epsilon for Adam optimizer.")
    parser.add_argument("--use_memory", action="store_true", help="load dataset to memory to accelerate.")
    parser.add_argument("--max_length", default=512, type=int, help="max length of dataset")
    parser.add_argument("--data_dir", type=str, help="train data path")
    parser.add_argument("--save_path", type=str, help="train data path")
    parser.add_argument("--save_topk", default=1, type=int, help="save topk checkpoint")
    parser.add_argument("--warmup_proportion", default=0.1, type=float, help="Proportion of training to perform linear learning rate warmup for.")
    parser.add_argument("--hidden_dropout_prob", type=float, default=0.1, )
    parser.add_argument("--seed", type=int, default=2333)
    parser.add_argument("--optimizer", type=str, default="adamw")
    parser.add_argument("--classifier", type=str, default="single")
    parser.add_argument("--no_lr_scheduler", action="store_true")
    parser.add_argument("--save_ner_prediction", action="store_true", help="only work for test.")
    parser.add_argument("--path_to_model_hparams_file", default="", type=str, help="use for evaluation")
    parser.add_argument("--checkpoint_path", default="", type=str, help="use for evaluation.")
    parser.add_argument("--file_name", default="", type=str, help="use for truncated sets.")
    parser.add_argument("--task_name", default="ner", type=str, help="load different dataset")
    
    return parser


def main():
    parser = get_parser()
    #è¿™é‡Œè°ƒç”¨äº†ä¸€ä¸ªåä¸ºget_parser()çš„å‡½æ•°ï¼Œè¯¥å‡½æ•°è¿”å›žä¸€ä¸ªargparse.ArgumentParserå¯¹è±¡ï¼Œç”¨äºŽè§£æžå‘½ä»¤è¡Œå‚æ•°ã€‚

    parser = Trainer.add_argparse_args(parser)
    #å°†Trainerç±»ä¸­çš„å‚æ•°æ·»åŠ åˆ°ä¹‹å‰åˆ›å»ºçš„argparse.ArgumentParserå¯¹è±¡ä¸­ã€‚

    args = parser.parse_args()
    
    args_dict = vars(args)
    for i in args_dict:
        print("%så‚æ•°çš„å€¼ä¸ºï¼š%s"%(i, str(args_dict[i])))
     #è§£æžå‘½ä»¤è¡Œå‚æ•°ï¼Œå°†å®ƒä»¬ä¿å­˜åœ¨argså˜é‡ä¸­ã€‚

    model = WeiboTask(args)
    #åˆ›å»ºä¸€ä¸ªåä¸ºWeiboTaskçš„æ¨¡åž‹ï¼Œå¹¶å°†è§£æžåŽçš„å‚æ•°argsä¼ é€’ç»™è¯¥æ¨¡åž‹åˆå§‹åŒ–å‡½æ•°ï¼Œä»¥ä¾¿è¿›è¡Œæ¨¡åž‹çš„é…ç½®å’Œåˆå§‹åŒ–ã€‚


    checkpoint_callback = ModelCheckpoint(
        filepath=os.path.join(args.save_path, "checkpoint", "{epoch}",),
        #filepath: è¡¨ç¤ºä¿å­˜æ¨¡åž‹æ£€æŸ¥ç‚¹çš„æ–‡ä»¶è·¯å¾„ã€‚

        save_top_k=args.save_topk,
        #save_top_k: è¡¨ç¤ºè¦ä¿å­˜æœ€å¥½çš„å‡ ä¸ªæ¨¡åž‹æ£€æŸ¥ç‚¹ã€‚

        save_last=False,
        #save_last: è¡¨ç¤ºæ˜¯å¦ä¿å­˜æœ€åŽä¸€ä¸ªï¼ˆæœ€åŽçš„epochï¼‰æ¨¡åž‹æ£€æŸ¥ç‚¹ã€‚åœ¨è¿™é‡Œï¼ŒFalseè¡¨ç¤ºä¸ä¿å­˜æœ€åŽä¸€ä¸ªæ£€æŸ¥ç‚¹ï¼Œå³åªä¿å­˜save_top_kä¸ªæœ€å¥½çš„æ£€æŸ¥ç‚¹ã€‚

        monitor="val_f1",
        #monitor: è¡¨ç¤ºè¦ç›‘è§†çš„æŒ‡æ ‡åç§°ï¼Œç”¨äºŽå†³å®šä½•æ—¶ä¿å­˜æ¨¡åž‹æ£€æŸ¥ç‚¹ã€‚åœ¨è¿™é‡Œï¼Œ"val_f1"è¡¨ç¤ºæ¨¡åž‹å°†ä¼šåœ¨éªŒè¯é›†ä¸Šçš„val_f1æŒ‡æ ‡ä¸Šè¿›è¡Œç›‘è§†ï¼Œä»¥ä¾¿ç¡®å®šæ˜¯å¦ä¿å­˜æ£€æŸ¥ç‚¹ã€‚

        mode="max",
        #mode: è¡¨ç¤ºæ¨¡åž‹æ£€æŸ¥ç‚¹ä¿å­˜çš„æ–¹å¼ã€‚åœ¨è¿™é‡Œï¼Œ"max"è¡¨ç¤ºè¦ä¿å­˜å…·æœ‰æœ€å¤§val_f1å€¼çš„æ£€æŸ¥ç‚¹ã€‚

        verbose=True,
        #verbose: è¡¨ç¤ºæ˜¯å¦åœ¨ä¿å­˜æ£€æŸ¥ç‚¹æ—¶è¾“å‡ºä¸€äº›é¢å¤–çš„ä¿¡æ¯ã€‚åœ¨è¿™é‡Œï¼ŒTrueè¡¨ç¤ºåœ¨ä¿å­˜æ£€æŸ¥ç‚¹æ—¶è¾“å‡ºä¿¡æ¯ã€‚

        period=-1,
        #period: è¡¨ç¤ºä¿å­˜æ£€æŸ¥ç‚¹çš„é—´éš”ï¼Œå³æ¯éš”å¤šå°‘ä¸ªepochä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹ã€‚åœ¨è¿™é‡Œï¼Œ-1è¡¨ç¤ºæ¯ä¸ªepochéƒ½ä¼šä¿å­˜æ£€æŸ¥ç‚¹ã€‚

    )
    #åˆ›å»ºä¸€ä¸ªModelCheckpointå›žè°ƒå‡½æ•°ï¼Œè¯¥å‡½æ•°ç”¨äºŽåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®šæœŸä¿å­˜æ¨¡åž‹æ£€æŸ¥ç‚¹ã€‚æ£€æŸ¥ç‚¹ä¼šæ ¹æ®éªŒè¯é›†ä¸Šçš„val_f1æŒ‡æ ‡æ¥ä¿å­˜ï¼Œåªä¿å­˜è¾¾åˆ°æœ€é«˜f1å€¼çš„æ£€æŸ¥ç‚¹ã€‚

    logger = TensorBoardLogger(
        save_dir=args.save_path,
        name='log')

    # save args
    with open(os.path.join(args.save_path, "checkpoint", "args.json"), "w") as f:
        #è¿™é‡Œä½¿ç”¨os.path.join()å‡½æ•°å°†æ–‡ä»¶è·¯å¾„æ‹¼æŽ¥èµ·æ¥ã€‚

        args_dict = args.__dict__
        #è¿™é‡Œä½¿ç”¨args.__dict__å°†å‘½ä»¤è¡Œå‚æ•°argsè½¬æ¢æˆä¸€ä¸ªå­—å…¸ç±»åž‹çš„å¯¹è±¡args_dictã€‚

        del args_dict["tpu_cores"]
        #è¿™é‡Œåˆ é™¤äº†args_dictå­—å…¸ä¸­çš„ä¸€ä¸ªé”®å€¼å¯¹ï¼Œå³"tpu_cores"ã€‚

        json.dump(args_dict, f, indent=4)
        #ä½¿ç”¨json.dump()å°†args_dictå­—å…¸çš„å†…å®¹ä»¥JSONæ ¼å¼å†™å…¥æ–‡ä»¶fä¸­ã€‚indent=4å‚æ•°æ˜¯ä¸ºäº†ä½¿å¾—å†™å…¥çš„JSONæ–‡ä»¶æ›´åŠ æ˜“è¯»ï¼Œå®ƒè¡¨ç¤ºä½¿ç”¨4ä¸ªç©ºæ ¼æ¥ç¼©è¿›JSONæ•°æ®ã€‚
    #è¿™æ®µä»£ç çš„ä½œç”¨æ˜¯å°†è§£æžåŽçš„å‘½ä»¤è¡Œå‚æ•°ä¿å­˜ä¸ºJSONæ ¼å¼çš„æ–‡ä»¶ï¼Œä»¥ä¾¿åŽç»­è·Ÿè¸ªå’ŒæŸ¥çœ‹è®­ç»ƒæ—¶ä½¿ç”¨çš„å‚æ•°é…ç½®ã€‚

    trainer = Trainer.from_argparse_args(args,
                                         #argsåŒ…å«äº†æ¨¡åž‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„å„ç§å‚æ•°ï¼Œå¦‚å­¦ä¹ çŽ‡ã€æ‰¹é‡å¤§å°ã€è®­ç»ƒè½®æ•°ç­‰ç­‰ã€‚

                                         checkpoint_callback=checkpoint_callback,
                                         #è¿™æ˜¯ä¸€ä¸ªå…³é”®å­—å‚æ•°ï¼ŒæŒ‡å®šäº†ä¹‹å‰åˆ›å»ºçš„ModelCheckpointå›žè°ƒå‡½æ•°checkpoint_callbackã€‚

                                         logger=logger,
                                         #logger=logger: è¿™æ˜¯å¦ä¸€ä¸ªå…³é”®å­—å‚æ•°ï¼ŒæŒ‡å®šäº†ä¹‹å‰åˆ›å»ºçš„TensorBoardLoggerå¯¹è±¡loggerã€‚TensorBoardLoggerç”¨äºŽè®°å½•è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ—¥å¿—ä¿¡æ¯

                                         deterministic=True)
                                        #deterministic=True: è¿™æ˜¯ä¸€ä¸ªå…³é”®å­—å‚æ•°ï¼Œç”¨äºŽè®¾ç½®æ˜¯å¦ä½¿ç”¨éšæœºæ•°ç§å­ä½¿å¾—è®­ç»ƒè¿‡ç¨‹å…·æœ‰ç¡®å®šæ€§ã€‚å¦‚æžœè®¾ç½®ä¸ºTrueï¼Œåˆ™è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨çš„éšæœºæ•°ç§å­å°†è¢«å›ºå®šï¼Œä»¥ä¾¿äºŽç»“æžœçš„å¤çŽ°ã€‚
    #è¿™æ®µä»£ç ä½¿ç”¨PyTorch Lightningåº“ä¸­çš„Trainer.from_argparse_args()æ–¹æ³•æ¥åˆ›å»ºä¸€ä¸ªTrainerå¯¹è±¡ï¼Œç”¨äºŽç®¡ç†æ·±åº¦å­¦ä¹ æ¨¡åž‹çš„è®­ç»ƒè¿‡ç¨‹ã€‚

    trainer.fit(model)
    #ä½¿ç”¨Trainerå¯¹è±¡å¯¹æ¨¡åž‹è¿›è¡Œè®­ç»ƒï¼Œè¿™é‡Œmodelæ˜¯ä¹‹å‰åˆ›å»ºçš„WeiboTaskæ¨¡åž‹ã€‚

    # after training, use the model checkpoint which achieves the best f1 score on dev set to compute the f1 on test set.
    best_f1_on_dev, path_to_best_checkpoint = find_best_checkpoint_on_dev(args.save_path)
    #æ‰¾åˆ°åœ¨å¼€å‘é›†ä¸ŠèŽ·å¾—æœ€ä½³f1åˆ†æ•°çš„æ£€æŸ¥ç‚¹ï¼Œè¿™æ˜¯ä¸ºäº†åœ¨æµ‹è¯•é›†ä¸Šä½¿ç”¨è¡¨çŽ°æœ€å¥½çš„æ¨¡åž‹ã€‚

    model.result_logger.info("=&"*20)
    #è¿™è¡Œä»£ç é€šè¿‡result_loggerå¯¹è±¡è®°å½•ä¸€æ¡æ—¥å¿—ä¿¡æ¯ã€‚è¿™é‡Œçš„æ—¥å¿—ä¿¡æ¯æ˜¯ä¸€ä¸ªåŒ…å«20ä¸ª"=&"çš„å­—ç¬¦ä¸²ï¼Œé‡å¤20æ¬¡ã€‚

    print("=="*50)

    model.result_logger.info(f"Best F1 on DEV is {best_f1_on_dev}")
    #è¿™è¡Œä»£ç é€šè¿‡result_loggerå¯¹è±¡è®°å½•ä¸€æ¡æ—¥å¿—ä¿¡æ¯ï¼Œè¾“å‡ºå¼€å‘é›†ä¸Šçš„æœ€ä½³F1å€¼ã€‚

    print(f"Best F1 on DEV is {best_f1_on_dev}")

    model.result_logger.info(f"Best checkpoint on DEV set is {path_to_best_checkpoint}")
    #: è¿™è¡Œä»£ç é€šè¿‡result_loggerå¯¹è±¡è®°å½•ä¸€æ¡æ—¥å¿—ä¿¡æ¯ï¼Œè¾“å‡ºå¼€å‘é›†ä¸Šçš„æœ€ä½³æ¨¡åž‹æ£€æŸ¥ç‚¹çš„è·¯å¾„ã€‚

    print(f"Best checkpoint on DEV set is {path_to_best_checkpoint}")
    checkpoint = torch.load(path_to_best_checkpoint)
    #åŠ è½½å¼€å‘é›†ä¸Šè¡¨çŽ°æœ€å¥½çš„æ¨¡åž‹æ£€æŸ¥ç‚¹ã€‚

    model.load_state_dict(checkpoint['state_dict'])
    #å°†æ¨¡åž‹çš„å‚æ•°åŠ è½½ä¸ºå¼€å‘é›†ä¸Šè¡¨çŽ°æœ€å¥½çš„å‚æ•°ã€‚

    trainer.test(model)
    #ä½¿ç”¨æµ‹è¯•é›†å¯¹æ¨¡åž‹è¿›è¡Œæµ‹è¯•ï¼Œè¯„ä¼°æ¨¡åž‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½ã€‚

    model.result_logger.info("=&"*20)
    print("=="*50)


def find_best_checkpoint_on_dev(output_dir: str, log_file: str = "eval_result_log.txt"):
    with open(os.path.join(output_dir, log_file)) as f:
        log_lines = f.readlines()

    F1_PATTERN=re.compile(r"val_f1 reached \d+\.\d* \(best")
    # val_f1 reached 0.00000 (best 0.00000)
    CKPT_PATTERN=re.compile(r"saving model to \S+ as top")
    checkpoint_info_lines = []
    for log_line in log_lines:
        if "saving model to" in log_line:
            checkpoint_info_lines.append(log_line)
    # example of log line
    # Epoch 00000: val_f1 reached 0.00000 (best 0.00000), saving model to /data/xiaoya/outputs/glyce/0117/debug_5_12_2e-5_0.001_0.001_275_0.1_1_0.25/checkpoint/epoch=0.ckpt as top 20
    best_f1_on_dev = 0
    best_checkpoint_on_dev = 0
    for checkpoint_info_line in checkpoint_info_lines:
        current_f1 = float(re.findall(F1_PATTERN, checkpoint_info_line)[0].replace("val_f1 reached ", "").replace(" (best", ""))
        current_ckpt = re.findall(CKPT_PATTERN, checkpoint_info_line)[0].replace("saving model to ", "").replace(" as top", "")

        if current_f1 >= best_f1_on_dev:
            best_f1_on_dev = current_f1
            best_checkpoint_on_dev = current_ckpt

    return best_f1_on_dev, best_checkpoint_on_dev


def evaluate():
    parser = get_parser()
    parser = Trainer.add_argparse_args(parser)
    args = parser.parse_args()

    model = WeiboTask.load_from_checkpoint(checkpoint_path=args.checkpoint_path,
                                                            hparams_file=args.path_to_model_hparams_file,
                                                            map_location=None,
                                                            batch_size=1)
    trainer = Trainer.from_argparse_args(args, deterministic=True)

    trainer.test(model)


if __name__ == "__main__":
    from multiprocessing import freeze_support

    freeze_support()
    main()


